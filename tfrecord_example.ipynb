{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is TFRecord file\n",
    "\n",
    "The tfrecord file is recommended file format in tensorflow as introduced in (https://www.tensorflow.org/api_guides/python/reading_data#Reading_from_files).\n",
    "\n",
    "If you prepare tfrecords file, you can load it using tf.contrib.data.Dataset API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = tf.contrib.data.TFRecordDataset('filename.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making tfrecord file, you should understand protocol buffer since the tfrecord format consits of it.\n",
    "\n",
    "According to [Google protocol buffer](https://developers.google.com/protocol-buffers/), protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializaing structured data, like XML but smaller, faster, and simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply saying, protocol buffer can be regarded as container class to which input data is saved.\n",
    "\n",
    "The order of converting data to tfrecord file using protocol buffer class could be described as follows:\n",
    "\n",
    "1. Instantiate protocol buffer class\n",
    "2. fill in the instance field with the data\n",
    "3. serialize it with its instance method\n",
    "4. convert and save to tfrecord file with tf.python_io.TFRecordWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's examine the structure of protocol buffer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example has single field 'features' of 'Features' field type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message Example {\n",
    "  Features features = 1;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features has single field 'feature' of 'map&lt;string, Feature&gt; field type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message Features {\n",
    "  // Map from feature name to feature.\n",
    "  map<string, Feature> feature = 1;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature has single field which could be one of the following:\n",
    "\n",
    "1. bytes_list of BytesList type\n",
    "2. float_list of FloatList type\n",
    "3. int64_list of Int64List type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Containers for non-sequential data.\n",
    "message Feature {\n",
    "  // Each feature can be exactly one kind.\n",
    "  oneof kind {\n",
    "    BytesList bytes_list = 1;\n",
    "    FloatList float_list = 2;\n",
    "    Int64List int64_list = 3;\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of them has following definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Containers to hold repeated fundamental values.\n",
    "message BytesList {\n",
    "  repeated bytes value = 1;\n",
    "}\n",
    "message FloatList {\n",
    "  repeated float value = 1 [packed = true];\n",
    "}\n",
    "message Int64List {\n",
    "  repeated int64 value = 1 [packed = true];\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the inner most basic field we should fill in is 'value' field.\n",
    "\n",
    "Now, how to fill in this field?\n",
    "\n",
    "Just reference field name from outermost to innermost and assign the value.\n",
    "\n",
    "**note)  'map' type works like dictionary, and 'repeated' type works like python sequences.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = tf.train.Example() # instanciate Example protocol buffer class which have features field.\n",
    "example.features.feature['feature_name'].float_list.value[:] = [10,20,30] # assign value (python sequence).\n",
    "\n",
    "# There are multiple ways to assign value\n",
    "example.features.feature['feature_name'].float_list.value.append(10) # append one value\n",
    "example.features.feature['feature_name'].float_list.value.extend([20,30]) # append list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make practical example.\n",
    "\n",
    "** (code run on 2013 macbook air) **\n",
    "\n",
    "Make tfrecord file from MNIST numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.contrib.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n",
      "(60000,) uint8\n",
      "(10000, 28, 28) uint8\n",
      "(10000,) uint8\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_train.dtype)\n",
    "print(y_train.shape, y_train.dtype)\n",
    "print(x_test.shape, x_test.dtype)\n",
    "print(y_test.shape, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(float) # cast from int to float\n",
    "x_test = x_test.astype(float)\n",
    "\n",
    "x_train = x_train / 255 # scale to 0-1 float value\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28) # reshape from (samples, height, width) to (samples, feature dimensions)\n",
    "x_test = x_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120s elapsed\n"
     ]
    }
   ],
   "source": [
    "train_writer = tf.python_io.TFRecordWriter(path='./train.tfrecords')\n",
    "test_writer = tf.python_io.TFRecordWriter(path='./test.tfrecords')\n",
    "\n",
    "start = time.time()\n",
    "for x, y in zip(x_train, y_train):\n",
    "    train_exam = tf.train.Example()\n",
    "    train_exam.features.feature['image'].float_list.value[:] = x # 28*28 length float list\n",
    "    train_exam.features.feature['label'].int64_list.value.append(y) # single int value\n",
    "    train_writer.write(train_exam.SerializeToString())\n",
    "for x, y in zip(x_test, y_test):\n",
    "    test_exam = tf.train.Example()\n",
    "    test_exam.features.feature['image'].float_list.value[:] = x\n",
    "    test_exam.features.feature['label'].int64_list.value.append(y)\n",
    "    test_writer.write(test_exam.SerializeToString())\n",
    "    \n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "print('{:.0f}s elapsed'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have mnist train, test dataset in tfrecord file format.\n",
    "\n",
    "Build simple neural network and evaluate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = tf.placeholder(dtype=tf.string, shape=[])\n",
    "dataset_train = tf.contrib.data.TFRecordDataset(filename)\n",
    "\n",
    "def _parse_function(example_serialized):\n",
    "    features = {'image': tf.FixedLenFeature(shape=[28*28], dtype=tf.float32),\n",
    "               'label': tf.FixedLenFeature(shape=[], dtype=tf.int64)}\n",
    "    parsed_features = tf.parse_single_example(example_serialized, features)\n",
    "    return parsed_features['image'], parsed_features['label']\n",
    "\n",
    "dataset = dataset_train.map(_parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(60000)\n",
    "dataset = dataset.batch(128)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_batched = iterator.get_next()\n",
    "features = dataset_batched[0]\n",
    "labels = dataset_batched[1]\n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "training = tf.placeholder(dtype=tf.bool, shape=[])\n",
    "\n",
    "outputs = tf.layers.dense(features, 512, activation=tf.nn.relu)\n",
    "outputs = tf.layers.dropout(outputs, rate=1-keep_prob, training=training)\n",
    "outputs = tf.layers.dense(outputs, 512, activation=tf.nn.relu)\n",
    "outputs = tf.layers.dropout(outputs, rate=1-keep_prob, training=training)\n",
    "logits = tf.layers.dense(outputs, 10)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "update_step = optimizer.minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), labels), tf.float32))\n",
    "\n",
    "global_initializer = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, \n",
      "\ttrain_acc=0.917, train_loss=0.273, time=14s\n",
      "\ttest_acc=0.967, test_loss=0.108, time=1s\n",
      "Epoch 2/10, \n",
      "\ttrain_acc=0.965, train_loss=0.116, time=13s\n",
      "\ttest_acc=0.973, test_loss=0.085, time=1s\n",
      "Epoch 3/10, \n",
      "\ttrain_acc=0.974, train_loss=0.084, time=13s\n",
      "\ttest_acc=0.977, test_loss=0.073, time=1s\n",
      "Epoch 4/10, \n",
      "\ttrain_acc=0.979, train_loss=0.067, time=12s\n",
      "\ttest_acc=0.979, test_loss=0.068, time=1s\n",
      "Epoch 5/10, \n",
      "\ttrain_acc=0.982, train_loss=0.057, time=14s\n",
      "\ttest_acc=0.982, test_loss=0.059, time=1s\n",
      "Epoch 6/10, \n",
      "\ttrain_acc=0.984, train_loss=0.050, time=15s\n",
      "\ttest_acc=0.980, test_loss=0.072, time=1s\n",
      "Epoch 7/10, \n",
      "\ttrain_acc=0.986, train_loss=0.044, time=13s\n",
      "\ttest_acc=0.982, test_loss=0.060, time=1s\n",
      "Epoch 8/10, \n",
      "\ttrain_acc=0.988, train_loss=0.037, time=13s\n",
      "\ttest_acc=0.981, test_loss=0.066, time=1s\n",
      "Epoch 9/10, \n",
      "\ttrain_acc=0.988, train_loss=0.035, time=15s\n",
      "\ttest_acc=0.982, test_loss=0.066, time=1s\n",
      "Epoch 10/10, \n",
      "\ttrain_acc=0.989, train_loss=0.034, time=13s\n",
      "\ttest_acc=0.982, test_loss=0.068, time=1s\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(global_initializer)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        train_acc, train_loss = [], []\n",
    "        sess.run(iterator.initializer, feed_dict={filename: './train.tfrecords'})\n",
    "        while True:\n",
    "            try:\n",
    "                _, acc, _loss = sess.run([update_step, accuracy, loss], feed_dict={keep_prob: 0.7, training: True})\n",
    "                train_acc.append(acc)\n",
    "                train_loss.append(_loss)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                log = 'Epoch {}/{}, \\n\\ttrain_acc={:.3f}, train_loss={:.3f}, time={:.0f}s'\n",
    "                print(log.format(epoch+1, epochs, np.mean(train_acc), np.mean(train_loss), time.time()-start))\n",
    "                \n",
    "                start = time.time()\n",
    "                test_acc, test_loss = [], []\n",
    "                sess.run(iterator.initializer, feed_dict={filename: './test.tfrecords'})\n",
    "                while True:\n",
    "                    try:\n",
    "                        acc, _loss = sess.run([accuracy, loss], feed_dict={keep_prob: 1.0, training: False})\n",
    "                        test_acc.append(acc)\n",
    "                        test_loss.append(_loss)\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        log = '\\ttest_acc={:.3f}, test_loss={:.3f}, time={:.0f}s'\n",
    "                        print(log.format(np.mean(test_acc), np.mean(test_loss), time.time()-start))\n",
    "                        break\n",
    "                \n",
    "                break # go to next epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
